models:
  c4ai-command-r-plus:
    model_family: c4ai-command-r
    model_variant: plus
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: c4ai/command-r-plus
  c4ai-command-r-plus-08-2024:
    model_family: c4ai-command-r
    model_variant: plus-08-2024
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: c4ai/command-r-plus-08-2024
  c4ai-command-r-08-2024:
    model_family: c4ai-command-r
    model_variant: 08-2024
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: c4ai/command-r-08-2024
  CodeLlama-7b-hf:
    model_family: CodeLlama
    model_variant: 7b-hf
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/CodeLlama-7b-hf
  CodeLlama-7b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 7b-Instruct-hf
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/CodeLlama-7b-Instruct-hf
  CodeLlama-13b-hf:
    model_family: CodeLlama
    model_variant: 13b-hf
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/CodeLlama-13b-hf
  CodeLlama-13b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 13b-Instruct-hf
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/CodeLlama-13b-Instruct-hf
  CodeLlama-34b-hf:
    model_family: CodeLlama
    model_variant: 34b-hf
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/CodeLlama-34b-hf
  CodeLlama-34b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 34b-Instruct-hf
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/CodeLlama-34b-Instruct-hf
  CodeLlama-70b-hf:
    model_family: CodeLlama
    model_variant: 70b-hf
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/CodeLlama-70b-hf
  CodeLlama-70b-Instruct-hf:
    model_family: CodeLlama
    model_variant: 70b-Instruct-hf
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/CodeLlama-70b-Instruct-hf
  dbrx-instruct:
    model_family: dbrx
    model_variant: instruct
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 100352
    max_model_len: 32000
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: databricks/dbrx-instruct
  gemma-2-9b:
    model_family: gemma-2
    model_variant: 9b
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: google/gemma-2-9b
  gemma-2-9b-it:
    model_family: gemma-2
    model_variant: 9b-it
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: google/gemma-2-9b-it
  gemma-2-27b:
    model_family: gemma-2
    model_variant: 27b
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: google/gemma-2-27b
  gemma-2-27b-it:
    model_family: gemma-2
    model_variant: 27b-it
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: google/gemma-2-27b-it
  Llama-2-7b-hf:
    model_family: Llama-2
    model_variant: 7b-hf
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-2-7b-hf
  Llama-2-7b-chat-hf:
    model_family: Llama-2
    model_variant: 7b-chat-hf
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-2-7b-chat-hf
  Llama-2-13b-hf:
    model_family: Llama-2
    model_variant: 13b-hf
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-2-13b-hf
  Llama-2-13b-chat-hf:
    model_family: Llama-2
    model_variant: 13b-chat-hf
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-2-13b-chat-hf
  Llama-2-70b-hf:
    model_family: Llama-2
    model_variant: 70b-hf
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-2-70b-hf
  Llama-2-70b-chat-hf:
    model_family: Llama-2
    model_variant: 70b-chat-hf
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-2-70b-chat-hf
  llava-1.5-7b-hf:
    model_family: llava-1.5
    model_variant: 7b-hf
    model_type: VLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: llava-hf/llava-1.5-7b-hf
  llava-1.5-13b-hf:
    model_family: llava-1.5
    model_variant: 13b-hf
    model_type: VLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: llava-hf/llava-1.5-13b-hf
  llava-v1.6-mistral-7b-hf:
    model_family: llava-v1.6
    model_variant: mistral-7b-hf
    model_type: VLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: llava-hf/llava-v1.6-mistral-7b
  llava-v1.6-34b-hf:
    model_family: llava-v1.6
    model_variant: 34b-hf
    model_type: VLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 64064
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: llava-hf/llava-v1.6-34b
  Meta-Llama-3-8B:
    model_family: Meta-Llama-3
    model_variant: 8B
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Meta-Llama-3-8B
  Meta-Llama-3-8B-Instruct:
    model_family: Meta-Llama-3
    model_variant: 8B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Meta-Llama-3-8B-Instruct
  Meta-Llama-3-70B:
    model_family: Meta-Llama-3
    model_variant: 70B
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Meta-Llama-3-70B
  Meta-Llama-3-70B-Instruct:
    model_family: Meta-Llama-3
    model_variant: 70B-Instruct
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Meta-Llama-3-70B-Instruct
  Meta-Llama-3.1-8B:
    model_family: Meta-Llama-3.1
    model_variant: 8B
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.1-8B
  Meta-Llama-3.1-8B-Instruct:
    model_family: Meta-Llama-3.1
    model_variant: 8B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.1-8B-Instruct
  Meta-Llama-3.1-70B:
    model_family: Meta-Llama-3.1
    model_variant: 70B
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.1-70B
  Meta-Llama-3.1-70B-Instruct:
    model_family: Meta-Llama-3.1
    model_variant: 70B-Instruct
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.1-70B-Instruct
  Meta-Llama-3.1-405B-Instruct:
    model_family: Meta-Llama-3.1
    model_variant: 405B-Instruct
    model_type: LLM
    num_gpus: 4
    num_nodes: 2
    vocab_size: 128256
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.1-405B-Instruct
  Mistral-7B-v0.1:
    model_family: Mistral
    model_variant: 7B-v0.1
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: mistralai/Mistral-7B-v0.1
  Mistral-7B-Instruct-v0.1:
    model_family: Mistral
    model_variant: 7B-Instruct-v0.1
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: mistralai/Mistral-7B-Instruct-v0.1
  Mistral-7B-Instruct-v0.2:
    model_family: Mistral
    model_variant: 7B-Instruct-v0.2
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: mistralai/Mistral-7B-Instruct-v0.2
  Mistral-7B-v0.3:
    model_family: Mistral
    model_variant: 7B-v0.3
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32768
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: mistralai/Mistral-7B-v0.3
  Mistral-7B-Instruct-v0.3:
    model_family: Mistral
    model_variant: 7B-Instruct-v0.3
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32768
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: mistralai/Mistral-7B-Instruct-v0.3
  Mistral-Large-Instruct-2407:
    model_family: Mistral
    model_variant: Large-Instruct-2407
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 32768
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: mistralai/Mistral-Large-Instruct-2407
  Mistral-Large-Instruct-2411:
    model_family: Mistral
    model_variant: Large-Instruct-2411
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 32768
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: mistralai/Mistral-Large-Instruct-2411
  Mixtral-8x7B-Instruct-v0.1:
    model_family: Mixtral
    model_variant: 8x7B-Instruct-v0.1
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: mistralai/Mixtral-8x7B-Instruct-v0.1
  Mixtral-8x22B-v0.1:
    model_family: Mixtral
    model_variant: 8x22B-v0.1
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 32768
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: mistralai/Mixtral-8x22B-v0.1
  Mixtral-8x22B-Instruct-v0.1:
    model_family: Mixtral
    model_variant: 8x22B-Instruct-v0.1
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 32768
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: mistralai/Mixtral-8x22B-Instruct-v0.1
  Phi-3-medium-128k-instruct:
    model_family: Phi-3
    model_variant: medium-128k-instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32064
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: microsoft/Phi-3-medium-128k-instruct
  Phi-3-vision-128k-instruct:
    model_family: Phi-3
    model_variant: vision-128k-instruct
    model_type: VLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32064
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: microsoft/Phi-3-vision-128k-instruct
  Llama3-OpenBioLLM-70B:
    model_family: Llama3-OpenBioLLM
    model_variant: 70B
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: openbioai/Llama3-OpenBioLLM-70B
  Llama-3.1-Nemotron-70B-Instruct-HF:
    model_family: Llama-3.1-Nemotron
    model_variant: 70B-Instruct-HF
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: nvidia/Llama-3.1-Nemotron-70B-Instruct-HF
  Llama-3.2-1B:
    model_family: Llama-3.2
    model_variant: 1B
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.2-1B
  Llama-3.2-1B-Instruct:
    model_family: Llama-3.2
    model_variant: 1B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.2-1B-Instruct
  Llama-3.2-3B:
    model_family: Llama-3.2
    model_variant: 3B
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.2-3B
  Llama-3.2-3B-Instruct:
    model_family: Llama-3.2
    model_variant: 3B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.2-3B-Instruct
  Llama-3.2-11B-Vision:
    model_family: Llama-3.2
    model_variant: 11B-Vision
    model_type: VLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 4096
    max_num_seqs: 64
    pipeline_parallelism: false
    enforce_eager: true
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.2-11B-Vision
  Llama-3.2-11B-Vision-Instruct:
    model_family: Llama-3.2
    model_variant: 11B-Vision-Instruct
    model_type: VLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 4096
    max_num_seqs: 64
    pipeline_parallelism: false
    enforce_eager: true
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.2-11B-Vision-Instruct
  Llama-3.2-90B-Vision:
    model_family: Llama-3.2
    model_variant: 90B-Vision
    model_type: VLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 4096
    max_num_seqs: 32
    pipeline_parallelism: false
    enforce_eager: true
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.2-90B-Vision
  Llama-3.2-90B-Vision-Instruct:
    model_family: Llama-3.2
    model_variant: 90B-Vision-Instruct
    model_type: VLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 4096
    max_num_seqs: 32
    pipeline_parallelism: false
    enforce_eager: true
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.2-90B-Vision-Instruct
  Qwen2.5-0.5B-Instruct:
    model_family: Qwen2.5
    model_variant: 0.5B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-0.5B-Instruct
  Qwen2.5-1.5B-Instruct:
    model_family: Qwen2.5
    model_variant: 1.5B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-1.5B-Instruct
  Qwen2.5-3B-Instruct:
    model_family: Qwen2.5
    model_variant: 3B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-3B-Instruct
  Qwen2.5-7B-Instruct:
    model_family: Qwen2.5
    model_variant: 7B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-7B-Instruct
  Qwen2.5-14B-Instruct:
    model_family: Qwen2.5
    model_variant: 14B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-14B-Instruct
  Qwen2.5-32B-Instruct:
    model_family: Qwen2.5
    model_variant: 32B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-32B-Instruct
  Qwen2.5-72B-Instruct:
    model_family: Qwen2.5
    model_variant: 72B-Instruct
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-72B-Instruct
  Qwen2.5-Math-1.5B-Instruct:
    model_family: Qwen2.5
    model_variant: Math-1.5B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-Math-1.5B-Instruct
  Qwen2.5-Math-7B-Instruct:
    model_family: Qwen2.5
    model_variant: Math-7B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-Math-7B-Instruct
  Qwen2.5-Math-72B-Instruct:
    model_family: Qwen2.5
    model_variant: Math-72B-Instruct
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 16384
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-Math-72B-Instruct
  Qwen2.5-Coder-7B-Instruct:
    model_family: Qwen2.5
    model_variant: Coder-7B-Instruct
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-Coder-7B-Instruct
  Qwen2.5-Math-RM-72B:
    model_family: Qwen2.5
    model_variant: Math-RM-72B
    model_type: Reward_Modeling
    num_gpus: 2
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-Math-RM-72B
  QwQ-32B-Preview:
    model_family: QwQ
    model_variant: 32B-Preview
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Qwen/Qwen2.5-32B-Preview
  Pixtral-12B-2409:
    model_family: Pixtral
    model_variant: 12B-2409
    model_type: VLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 131072
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Pixtral/Pixtral-12B-2409
  e5-mistral-7b-instruct:
    model_family: e5
    model_variant: mistral-7b-instruct
    model_type: Text_Embedding
    num_gpus: 1
    num_nodes: 1
    vocab_size: 32000
    max_model_len: 4096
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: e5-mistral-7b-instruct
  bge-base-en-v1.5:
    model_family: bge
    model_variant: base-en-v1.5
    model_type: Text_Embedding
    num_gpus: 1
    num_nodes: 1
    vocab_size: 30522
    max_model_len: 512
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: bge-base-en-v1.5
  all-MiniLM-L6-v2:
    model_family: all-MiniLM
    model_variant: L6-v2
    model_type: Text_Embedding
    num_gpus: 1
    num_nodes: 1
    vocab_size: 30522
    max_model_len: 512
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: all-MiniLM-L6-v2
  InternVL2_5-26B:
    model_family: InternVL2_5
    model_variant: 26B
    model_type: VLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 92553
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: InternVL2_5-26B
  InternVL2_5-38B:
    model_family: InternVL2_5
    model_variant: 38B
    model_type: VLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 92553
    max_model_len: 32768
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: InternVL2_5-38B
  Aya-Expanse-32B:
    model_family: Aya-Expanse
    model_variant: 32B
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 256000
    max_model_len: 8192
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: Aya-Expanse-32B
  DeepSeek-R1-Distill-Llama-70B:
    model_family: DeepSeek-R1
    model_variant: 'Distill-Llama-70B '
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: DeepSeek-R1-Distill-Llama-70B
  DeepSeek-R1-Distill-Llama-8B:
    model_family: DeepSeek-R1
    model_variant: 'Distill-Llama-8B '
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: DeepSeek-R1-Distill-Llama-8B
  DeepSeek-R1-Distill-Qwen-32B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-32B
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: DeepSeek-R1-Distill-Qwen-32B
  DeepSeek-R1-Distill-Qwen-14B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-14B
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: DeepSeek-R1-Distill-Qwen-14B
  DeepSeek-R1-Distill-Qwen-7B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-7B
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: DeepSeek-R1-Distill-Qwen-7B
  DeepSeek-R1-Distill-Qwen-1.5B:
    model_family: DeepSeek-R1
    model_variant: Distill-Qwen-1.5B
    model_type: LLM
    num_gpus: 1
    num_nodes: 1
    vocab_size: 152064
    max_model_len: 131072
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: DeepSeek-R1-Distill-Qwen-1.5B
  Llama-3.3-70B-Instruct:
    model_family: Llama-3.3
    model_variant: 70B-Instruct
    model_type: LLM
    num_gpus: 2
    num_nodes: 1
    vocab_size: 128256
    max_model_len: 65536
    max_num_seqs: 256
    pipeline_parallelism: true
    enforce_eager: false
    time: 00:30:00
    partition: a100
    data_type: auto
    venv: apptainer
    log_dir: /projects/bbwb/rohan13/llm-inference/logs
    model_weights_parent_dir: /projects/bbwb/rohan13/llm-inference/model-weights
    huggingface_id: meta-llama/Llama-3.3-70B-Instruct
